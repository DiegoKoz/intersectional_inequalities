{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from libs.RaceDistribution import RaceDistribution\n",
    "from libs.LastNamesInference import LastNamesInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_papers = pd.read_csv('/data/datasets//WOS/US/US_papers.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imputing by the mean: 100%|██████████| 4129691/4129691 [00:19<00:00, 211828.55it/s]\n"
     ]
    }
   ],
   "source": [
    "first_authors = us_papers[us_papers.ordre==1].copy().reset_index(drop=True)\n",
    "lni = LastNamesInference(names = first_authors.nom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(918458,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_authors.nom.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_names = lni.lastnames_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prenom(prenom):\n",
    "    prenom = re.sub(r\"\\b(?:[A-Z]\\.)+(?!\\w)\",'',prenom)\n",
    "    prenom = re.sub(' +', ' ',prenom)\n",
    "    prenom = prenom.strip()\n",
    "    return prenom.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_names = [clean_prenom(x) for x in first_authors.Prenom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = [lni.clean_nom(x) for x in first_authors.nom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_names_wos =  [i for i in all_names if i in known_names]\n",
    "unknown_names_wos =  [i for i in all_names if i not in known_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "774381"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unknown_names_wos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18751548239323476"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unknown_names_wos)/len(all_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_race(us_papers):\n",
    "    first_authors = us_papers[us_papers.ordre==1].copy().reset_index(drop=True)\n",
    "    #first_authors = first_authors[['id_art','Prenom', 'nom']]\n",
    "    lni = LastNamesInference(names = first_authors.nom)\n",
    "    tqdm.pandas(desc=\"inferring race from lastnames\")\n",
    "    lastname_race_dist = first_authors.progress_apply(lambda x: lni.get_name_dist(lastname=x.nom), axis=1)\n",
    "    first_authors[lni.prob_order] = pd.DataFrame(lastname_race_dist.to_list())\n",
    "    first_authors = first_authors[['id_art','white', 'hispanic', 'black', 'asian']]\n",
    "    us_papers_race = us_papers.merge(first_authors, on ='id_art')\n",
    "\n",
    "    return us_papers_race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_ID</th>\n",
       "      <th>Annee_Bibliographique</th>\n",
       "      <th>yfp</th>\n",
       "      <th>id_art</th>\n",
       "      <th>Prenom</th>\n",
       "      <th>nom</th>\n",
       "      <th>ordre</th>\n",
       "      <th>nb_auteur</th>\n",
       "      <th>EDiscipline</th>\n",
       "      <th>ESpecialite</th>\n",
       "      <th>cit_rel_all_IAC</th>\n",
       "      <th>ordre_auteur</th>\n",
       "      <th>Province</th>\n",
       "      <th>disc_origin</th>\n",
       "      <th>spec_origin</th>\n",
       "      <th>count_origin</th>\n",
       "      <th>gender</th>\n",
       "      <th>cit_all_IAC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14427861</th>\n",
       "      <td>42690115</td>\n",
       "      <td>2015</td>\n",
       "      <td>2006</td>\n",
       "      <td>54196225</td>\n",
       "      <td>Rupal S.</td>\n",
       "      <td>Bhatt-RS</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Clinical Medicine</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>MA</td>\n",
       "      <td>Clinical Medicine</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cluster_ID  Annee_Bibliographique   yfp    id_art    Prenom  \\\n",
       "14427861    42690115                   2015  2006  54196225  Rupal S.   \n",
       "\n",
       "               nom  ordre  nb_auteur        EDiscipline ESpecialite  \\\n",
       "14427861  Bhatt-RS      2          3  Clinical Medicine      Cancer   \n",
       "\n",
       "          cit_rel_all_IAC  ordre_auteur Province        disc_origin  \\\n",
       "14427861              NaN             2       MA  Clinical Medicine   \n",
       "\n",
       "         spec_origin count_origin gender  cit_all_IAC  \n",
       "14427861      Cancer          NaN      F          NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_papers.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018    2299205\n",
       "2017    2214362\n",
       "2016    2205826\n",
       "2015    2117084\n",
       "2014    2047594\n",
       "2013    1983888\n",
       "2012    1902626\n",
       "2011    1705189\n",
       "2010    1536059\n",
       "2009    1455984\n",
       "2008    1403497\n",
       "2019     424019\n",
       "Name: Annee_Bibliographique, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_papers.Annee_Bibliographique.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = us_papers[['cluster_ID', 'id_art', 'Prenom', 'nom','ordre']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_authors = authors.loc[authors.ordre == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster_ID    1609107\n",
       "Prenom         255430\n",
       "nom            918458\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_authors[['cluster_ID', 'Prenom', 'nom']].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prenom(prenom):\n",
    "    prenom = re.sub(r\"\\b(?:[A-Z]\\.)+(?!\\w)\",'',prenom)\n",
    "    prenom = re.sub(' +', ' ',prenom)\n",
    "    prenom = prenom.strip()\n",
    "    return prenom.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_nom(nom):\n",
    "    nom = re.sub(r\"-[A-Z]+\",'',nom)\n",
    "    nom = re.sub(' +', ' ',nom)\n",
    "    nom = nom.strip()\n",
    "    return nom.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-1b44905f0940>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  first_authors['Prenom'] = first_authors.Prenom.apply(lambda x: clean_prenom(x))\n",
      "<ipython-input-20-1b44905f0940>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  first_authors['nom'] = first_authors.nom.apply(lambda x: clean_nom(x))\n"
     ]
    }
   ],
   "source": [
    "first_authors['Prenom'] = first_authors.Prenom.apply(lambda x: clean_prenom(x))\n",
    "first_authors['nom'] = first_authors.nom.apply(lambda x: clean_nom(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select first name\n",
    "prenom_list = first_authors.groupby(['cluster_ID'])['Prenom'].apply(lambda x: x.values).reset_index()\n",
    "prenom_list['Prenom'] = prenom_list.Prenom.apply(lambda x: list(filter(None,x)))\n",
    "prenom_list['Prenom_alt'] = prenom_list.Prenom.apply(lambda x: random.sample(x,1) if len(x)>0 else [''])\n",
    "prenom_list = prenom_list.explode('Prenom_alt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_authors = first_authors.merge(prenom_list[['cluster_ID','Prenom_alt']], how='left', on='cluster_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's still room for improvement, but I will use this list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get unique authors list\n",
    "unique_authors = first_authors.groupby(['cluster_ID', 'Prenom_alt', 'nom']).size().reset_index().rename(columns={0:'count'})\n",
    "\n",
    "unique_authors.drop('count',axis=1, inplace=True)\n",
    "unique_authors.columns = ['cluster_ID', 'Prenom','nom']\n",
    "\n",
    "# I remove duplicates. With the names and lastnames from the cluster this should not be necessary\n",
    "unique_authors = unique_authors.drop_duplicates('cluster_ID').reset_index(drop=True)\n",
    "\n",
    "unique_authors = unique_authors.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # exp 1 (sd)\n",
    "# rd1 = RaceDistribution(exp=1,mode = 'sd')\n",
    "# authors_dist_1 = unique_authors.progress_apply(lambda x: rd1.get_names_dist(x.Prenom, x.nom), axis=1)\n",
    "# new_cols = ['exp1_'+ name for name in rd1.prob_order]\n",
    "# unique_authors[new_cols] = pd.DataFrame(authors_dist_1.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1609107/1609107 [03:52<00:00, 6931.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# exp 2 (var) normalized given names\n",
    "rd2_norm = RaceDistribution(exp=2,mode = 'sd',normalized_firstnames=True)\n",
    "authors_dist_2_norm = unique_authors.progress_apply(lambda x: rd2_norm.get_names_dist(x.Prenom, x.nom), axis=1)\n",
    "new_cols = ['exp2_norm_'+ name for name in rd2_norm.prob_order]\n",
    "unique_authors[new_cols] = pd.DataFrame(authors_dist_2_norm.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1609107/1609107 [03:50<00:00, 6990.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# exp 2 (var) unnormalized given names\n",
    "rd2 = RaceDistribution(exp=2,mode = 'sd',normalized_firstnames=False)\n",
    "authors_dist_2 = unique_authors.progress_apply(lambda x: rd2.get_names_dist(x.Prenom, x.nom), axis=1)\n",
    "new_cols = ['exp2_notnorm_'+ name for name in rd2_norm.prob_order]\n",
    "unique_authors[new_cols] = pd.DataFrame(authors_dist_2.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1609107/1609107 [00:55<00:00, 28915.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Only lastname\n",
    "rd_lastname = RaceDistribution(mode = 'lastname')\n",
    "authors_dist_lastname = unique_authors.progress_apply(lambda x: rd_lastname.get_names_dist(x.Prenom, x.nom), axis=1)\n",
    "new_cols = ['lastname_notnorm_'+ name for name in rd2_norm.prob_order]\n",
    "unique_authors[new_cols] = pd.DataFrame(authors_dist_lastname.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1609107/1609107 [00:55<00:00, 28930.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Only first name\n",
    "rd_name = RaceDistribution(mode = 'name', normalized_firstnames= False)\n",
    "authors_dist_name = unique_authors.progress_apply(lambda x: rd_name.get_names_dist(x.Prenom, x.nom), axis=1)\n",
    "new_cols = ['name_notnorm_'+ name for name in rd2_norm.prob_order]\n",
    "unique_authors[new_cols] = pd.DataFrame(authors_dist_name.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1609107/1609107 [00:56<00:00, 28632.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Only first name normalized\n",
    "rd_name_norm = RaceDistribution(mode = 'name', normalized_firstnames=True)\n",
    "authors_dist_name_norm = unique_authors.progress_apply(lambda x: rd_name_norm.get_names_dist(x.Prenom, x.nom), axis=1)\n",
    "new_cols = ['name_norm_'+ name for name in rd2_norm.prob_order]\n",
    "unique_authors[new_cols] = pd.DataFrame(authors_dist_name_norm.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_ID</th>\n",
       "      <th>Prenom</th>\n",
       "      <th>nom</th>\n",
       "      <th>exp2_norm_white</th>\n",
       "      <th>exp2_norm_hispanic</th>\n",
       "      <th>exp2_norm_black</th>\n",
       "      <th>exp2_norm_asian</th>\n",
       "      <th>exp2_norm_other</th>\n",
       "      <th>exp2_notnorm_white</th>\n",
       "      <th>exp2_notnorm_hispanic</th>\n",
       "      <th>...</th>\n",
       "      <th>name_notnorm_white</th>\n",
       "      <th>name_notnorm_hispanic</th>\n",
       "      <th>name_notnorm_black</th>\n",
       "      <th>name_notnorm_asian</th>\n",
       "      <th>name_notnorm_other</th>\n",
       "      <th>name_norm_white</th>\n",
       "      <th>name_norm_hispanic</th>\n",
       "      <th>name_norm_black</th>\n",
       "      <th>name_norm_asian</th>\n",
       "      <th>name_norm_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>zeke</td>\n",
       "      <td>barger</td>\n",
       "      <td>0.894304</td>\n",
       "      <td>0.029825</td>\n",
       "      <td>0.033911</td>\n",
       "      <td>0.015908</td>\n",
       "      <td>0.026053</td>\n",
       "      <td>0.853548</td>\n",
       "      <td>0.032123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.51422</td>\n",
       "      <td>0.08226</td>\n",
       "      <td>0.11541</td>\n",
       "      <td>0.28117</td>\n",
       "      <td>0.00694</td>\n",
       "      <td>0.336222</td>\n",
       "      <td>0.162606</td>\n",
       "      <td>0.274602</td>\n",
       "      <td>0.182239</td>\n",
       "      <td>0.044330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>mia</td>\n",
       "      <td>divecha</td>\n",
       "      <td>0.178327</td>\n",
       "      <td>0.025214</td>\n",
       "      <td>0.103187</td>\n",
       "      <td>0.605588</td>\n",
       "      <td>0.087684</td>\n",
       "      <td>0.317416</td>\n",
       "      <td>0.024534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.63399</td>\n",
       "      <td>0.06536</td>\n",
       "      <td>0.22222</td>\n",
       "      <td>0.06536</td>\n",
       "      <td>0.01307</td>\n",
       "      <td>0.345928</td>\n",
       "      <td>0.107817</td>\n",
       "      <td>0.441234</td>\n",
       "      <td>0.035352</td>\n",
       "      <td>0.069669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>adam</td>\n",
       "      <td>faeth</td>\n",
       "      <td>0.927500</td>\n",
       "      <td>0.034439</td>\n",
       "      <td>0.009835</td>\n",
       "      <td>0.004287</td>\n",
       "      <td>0.023939</td>\n",
       "      <td>0.958658</td>\n",
       "      <td>0.018114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.96148</td>\n",
       "      <td>0.01989</td>\n",
       "      <td>0.00635</td>\n",
       "      <td>0.01016</td>\n",
       "      <td>0.00212</td>\n",
       "      <td>0.893983</td>\n",
       "      <td>0.055911</td>\n",
       "      <td>0.021485</td>\n",
       "      <td>0.009364</td>\n",
       "      <td>0.019257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>185</td>\n",
       "      <td>anna</td>\n",
       "      <td>pidgeon</td>\n",
       "      <td>0.825879</td>\n",
       "      <td>0.072110</td>\n",
       "      <td>0.049371</td>\n",
       "      <td>0.016753</td>\n",
       "      <td>0.035886</td>\n",
       "      <td>0.873732</td>\n",
       "      <td>0.042063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.84762</td>\n",
       "      <td>0.07439</td>\n",
       "      <td>0.02211</td>\n",
       "      <td>0.05378</td>\n",
       "      <td>0.00210</td>\n",
       "      <td>0.690918</td>\n",
       "      <td>0.183321</td>\n",
       "      <td>0.065584</td>\n",
       "      <td>0.043455</td>\n",
       "      <td>0.016723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>245</td>\n",
       "      <td>david</td>\n",
       "      <td>gochis</td>\n",
       "      <td>0.883263</td>\n",
       "      <td>0.081120</td>\n",
       "      <td>0.019563</td>\n",
       "      <td>0.009155</td>\n",
       "      <td>0.006899</td>\n",
       "      <td>0.925783</td>\n",
       "      <td>0.054305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92808</td>\n",
       "      <td>0.03223</td>\n",
       "      <td>0.01393</td>\n",
       "      <td>0.02393</td>\n",
       "      <td>0.00183</td>\n",
       "      <td>0.830267</td>\n",
       "      <td>0.087169</td>\n",
       "      <td>0.045349</td>\n",
       "      <td>0.021221</td>\n",
       "      <td>0.015994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609102</th>\n",
       "      <td>55572228</td>\n",
       "      <td>victoria</td>\n",
       "      <td>corum</td>\n",
       "      <td>0.777930</td>\n",
       "      <td>0.072631</td>\n",
       "      <td>0.092094</td>\n",
       "      <td>0.017055</td>\n",
       "      <td>0.040291</td>\n",
       "      <td>0.841290</td>\n",
       "      <td>0.049468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.83515</td>\n",
       "      <td>0.08424</td>\n",
       "      <td>0.03740</td>\n",
       "      <td>0.03704</td>\n",
       "      <td>0.00617</td>\n",
       "      <td>0.631293</td>\n",
       "      <td>0.192511</td>\n",
       "      <td>0.102878</td>\n",
       "      <td>0.027754</td>\n",
       "      <td>0.045563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609103</th>\n",
       "      <td>55572236</td>\n",
       "      <td>matthew</td>\n",
       "      <td>chadourne</td>\n",
       "      <td>0.847614</td>\n",
       "      <td>0.061196</td>\n",
       "      <td>0.041846</td>\n",
       "      <td>0.028875</td>\n",
       "      <td>0.020469</td>\n",
       "      <td>0.890408</td>\n",
       "      <td>0.044431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.97350</td>\n",
       "      <td>0.01019</td>\n",
       "      <td>0.00679</td>\n",
       "      <td>0.00783</td>\n",
       "      <td>0.00169</td>\n",
       "      <td>0.924248</td>\n",
       "      <td>0.029248</td>\n",
       "      <td>0.023459</td>\n",
       "      <td>0.007369</td>\n",
       "      <td>0.015675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609104</th>\n",
       "      <td>55572238</td>\n",
       "      <td>ekua</td>\n",
       "      <td>bentil</td>\n",
       "      <td>0.068283</td>\n",
       "      <td>0.053151</td>\n",
       "      <td>0.796380</td>\n",
       "      <td>0.015884</td>\n",
       "      <td>0.066302</td>\n",
       "      <td>0.154833</td>\n",
       "      <td>0.052108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.51422</td>\n",
       "      <td>0.08226</td>\n",
       "      <td>0.11541</td>\n",
       "      <td>0.28117</td>\n",
       "      <td>0.00694</td>\n",
       "      <td>0.336222</td>\n",
       "      <td>0.162606</td>\n",
       "      <td>0.274602</td>\n",
       "      <td>0.182239</td>\n",
       "      <td>0.044330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609105</th>\n",
       "      <td>55572281</td>\n",
       "      <td>ewen</td>\n",
       "      <td>kingsmith</td>\n",
       "      <td>0.562383</td>\n",
       "      <td>0.067266</td>\n",
       "      <td>0.265761</td>\n",
       "      <td>0.030603</td>\n",
       "      <td>0.073986</td>\n",
       "      <td>0.570747</td>\n",
       "      <td>0.061630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.51422</td>\n",
       "      <td>0.08226</td>\n",
       "      <td>0.11541</td>\n",
       "      <td>0.28117</td>\n",
       "      <td>0.00694</td>\n",
       "      <td>0.336222</td>\n",
       "      <td>0.162606</td>\n",
       "      <td>0.274602</td>\n",
       "      <td>0.182239</td>\n",
       "      <td>0.044330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609106</th>\n",
       "      <td>55572321</td>\n",
       "      <td></td>\n",
       "      <td>driest</td>\n",
       "      <td>0.616025</td>\n",
       "      <td>0.140659</td>\n",
       "      <td>0.114230</td>\n",
       "      <td>0.095371</td>\n",
       "      <td>0.033715</td>\n",
       "      <td>0.610024</td>\n",
       "      <td>0.116510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.51422</td>\n",
       "      <td>0.08226</td>\n",
       "      <td>0.11541</td>\n",
       "      <td>0.28117</td>\n",
       "      <td>0.00694</td>\n",
       "      <td>0.336222</td>\n",
       "      <td>0.162606</td>\n",
       "      <td>0.274602</td>\n",
       "      <td>0.182239</td>\n",
       "      <td>0.044330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1609107 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         cluster_ID    Prenom        nom  exp2_norm_white  exp2_norm_hispanic  \\\n",
       "0                12      zeke     barger         0.894304            0.029825   \n",
       "1                50       mia    divecha         0.178327            0.025214   \n",
       "2                57      adam      faeth         0.927500            0.034439   \n",
       "3               185      anna    pidgeon         0.825879            0.072110   \n",
       "4               245     david     gochis         0.883263            0.081120   \n",
       "...             ...       ...        ...              ...                 ...   \n",
       "1609102    55572228  victoria      corum         0.777930            0.072631   \n",
       "1609103    55572236   matthew  chadourne         0.847614            0.061196   \n",
       "1609104    55572238      ekua     bentil         0.068283            0.053151   \n",
       "1609105    55572281      ewen  kingsmith         0.562383            0.067266   \n",
       "1609106    55572321               driest         0.616025            0.140659   \n",
       "\n",
       "         exp2_norm_black  exp2_norm_asian  exp2_norm_other  \\\n",
       "0               0.033911         0.015908         0.026053   \n",
       "1               0.103187         0.605588         0.087684   \n",
       "2               0.009835         0.004287         0.023939   \n",
       "3               0.049371         0.016753         0.035886   \n",
       "4               0.019563         0.009155         0.006899   \n",
       "...                  ...              ...              ...   \n",
       "1609102         0.092094         0.017055         0.040291   \n",
       "1609103         0.041846         0.028875         0.020469   \n",
       "1609104         0.796380         0.015884         0.066302   \n",
       "1609105         0.265761         0.030603         0.073986   \n",
       "1609106         0.114230         0.095371         0.033715   \n",
       "\n",
       "         exp2_notnorm_white  exp2_notnorm_hispanic  ...  name_notnorm_white  \\\n",
       "0                  0.853548               0.032123  ...             0.51422   \n",
       "1                  0.317416               0.024534  ...             0.63399   \n",
       "2                  0.958658               0.018114  ...             0.96148   \n",
       "3                  0.873732               0.042063  ...             0.84762   \n",
       "4                  0.925783               0.054305  ...             0.92808   \n",
       "...                     ...                    ...  ...                 ...   \n",
       "1609102            0.841290               0.049468  ...             0.83515   \n",
       "1609103            0.890408               0.044431  ...             0.97350   \n",
       "1609104            0.154833               0.052108  ...             0.51422   \n",
       "1609105            0.570747               0.061630  ...             0.51422   \n",
       "1609106            0.610024               0.116510  ...             0.51422   \n",
       "\n",
       "         name_notnorm_hispanic  name_notnorm_black  name_notnorm_asian  \\\n",
       "0                      0.08226             0.11541             0.28117   \n",
       "1                      0.06536             0.22222             0.06536   \n",
       "2                      0.01989             0.00635             0.01016   \n",
       "3                      0.07439             0.02211             0.05378   \n",
       "4                      0.03223             0.01393             0.02393   \n",
       "...                        ...                 ...                 ...   \n",
       "1609102                0.08424             0.03740             0.03704   \n",
       "1609103                0.01019             0.00679             0.00783   \n",
       "1609104                0.08226             0.11541             0.28117   \n",
       "1609105                0.08226             0.11541             0.28117   \n",
       "1609106                0.08226             0.11541             0.28117   \n",
       "\n",
       "         name_notnorm_other  name_norm_white  name_norm_hispanic  \\\n",
       "0                   0.00694         0.336222            0.162606   \n",
       "1                   0.01307         0.345928            0.107817   \n",
       "2                   0.00212         0.893983            0.055911   \n",
       "3                   0.00210         0.690918            0.183321   \n",
       "4                   0.00183         0.830267            0.087169   \n",
       "...                     ...              ...                 ...   \n",
       "1609102             0.00617         0.631293            0.192511   \n",
       "1609103             0.00169         0.924248            0.029248   \n",
       "1609104             0.00694         0.336222            0.162606   \n",
       "1609105             0.00694         0.336222            0.162606   \n",
       "1609106             0.00694         0.336222            0.162606   \n",
       "\n",
       "         name_norm_black  name_norm_asian  name_norm_other  \n",
       "0               0.274602         0.182239         0.044330  \n",
       "1               0.441234         0.035352         0.069669  \n",
       "2               0.021485         0.009364         0.019257  \n",
       "3               0.065584         0.043455         0.016723  \n",
       "4               0.045349         0.021221         0.015994  \n",
       "...                  ...              ...              ...  \n",
       "1609102         0.102878         0.027754         0.045563  \n",
       "1609103         0.023459         0.007369         0.015675  \n",
       "1609104         0.274602         0.182239         0.044330  \n",
       "1609105         0.274602         0.182239         0.044330  \n",
       "1609106         0.274602         0.182239         0.044330  \n",
       "\n",
       "[1609107 rows x 28 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_authors.to_csv('../data/unique_authors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disciplines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imputing by the mean: 100%|██████████| 4129691/4129691 [00:19<00:00, 215023.81it/s]\n",
      "inferring race from lastnames: 100%|██████████| 4129691/4129691 [01:07<00:00, 61409.54it/s]\n"
     ]
    }
   ],
   "source": [
    "us_papers_race = infer_race(us_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "white       0.571391\n",
       "hispanic    0.055431\n",
       "black       0.071673\n",
       "asian       0.301505\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_papers_race.loc[:,['white','hispanic','black','asian']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cluster_ID', 'Annee_Bibliographique', 'yfp', 'id_art', 'Prenom', 'nom',\n",
       "       'ordre', 'nb_auteur', 'EDiscipline', 'ESpecialite', 'cit_rel_all_IAC',\n",
       "       'ordre_auteur', 'Province', 'disc_origin', 'spec_origin',\n",
       "       'count_origin', 'gender', 'cit_all_IAC', 'white', 'hispanic', 'black',\n",
       "       'asian'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_papers_race.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_papers_race = us_papers_race[us_papers_race.ordre==1].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_papers_race.loc[us_papers_race.gender == 'f','gender'] = 'F'\n",
    "us_papers_race.loc[us_papers_race.gender == 'm','gender'] = 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_papers_race = us_papers_race[us_papers_race.gender.isin(['F','M'])].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I exclude 'Social Studies of Medicine' because there are too few obs\n",
    "us_papers_race = us_papers_race[us_papers_race.ESpecialite != 'Social Studies of Medicine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-09dbcb18f7b6>:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  discipline_gender_agg = us_papers_race.groupby(['EDiscipline','ESpecialite','gender'])['white', 'hispanic', 'black', 'asian'].agg({'count','mean'})\n"
     ]
    }
   ],
   "source": [
    "discipline_gender_agg = us_papers_race.groupby(['EDiscipline','ESpecialite','gender'])['white', 'hispanic', 'black', 'asian'].agg({'count','mean'})\n",
    "\n",
    "discipline_gender_agg = discipline_gender_agg.stack(level=0)\n",
    "\n",
    "#discipline_gender_agg.columns = ['EDiscipline', 'ESpecialite', 'gender', 'group', 'count', 'mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "discipline_gender_agg.index.names = ['EDiscipline', 'ESpecialite', 'gender', 'group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "discipline_gender_agg['freq'] = discipline_gender_agg['count']*discipline_gender_agg['mean']\n",
    "\n",
    "discipline_gender_agg = discipline_gender_agg.drop(['count','mean'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "discipline_gender_agg['joint_prob'] = discipline_gender_agg.freq/ discipline_gender_agg.freq.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "      <th>joint_prob</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDiscipline</th>\n",
       "      <th>ESpecialite</th>\n",
       "      <th>gender</th>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Arts</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">Fine Arts &amp; Architecture</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">F</th>\n",
       "      <th>asian</th>\n",
       "      <td>304.131391</td>\n",
       "      <td>0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black</th>\n",
       "      <td>258.206450</td>\n",
       "      <td>0.000080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hispanic</th>\n",
       "      <td>152.297873</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>1960.364286</td>\n",
       "      <td>0.000608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <th>asian</th>\n",
       "      <td>279.548700</td>\n",
       "      <td>0.000087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Social Sciences</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">Sociology</th>\n",
       "      <th>F</th>\n",
       "      <th>white</th>\n",
       "      <td>5858.704683</td>\n",
       "      <td>0.001816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">M</th>\n",
       "      <th>asian</th>\n",
       "      <td>1034.921559</td>\n",
       "      <td>0.000321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black</th>\n",
       "      <td>973.499659</td>\n",
       "      <td>0.000302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hispanic</th>\n",
       "      <td>651.249744</td>\n",
       "      <td>0.000202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>7375.329039</td>\n",
       "      <td>0.002286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1136 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 freq  \\\n",
       "EDiscipline     ESpecialite              gender group                   \n",
       "Arts            Fine Arts & Architecture F      asian      304.131391   \n",
       "                                                black      258.206450   \n",
       "                                                hispanic   152.297873   \n",
       "                                                white     1960.364286   \n",
       "                                         M      asian      279.548700   \n",
       "...                                                               ...   \n",
       "Social Sciences Sociology                F      white     5858.704683   \n",
       "                                         M      asian     1034.921559   \n",
       "                                                black      973.499659   \n",
       "                                                hispanic   651.249744   \n",
       "                                                white     7375.329039   \n",
       "\n",
       "                                                          joint_prob  \n",
       "EDiscipline     ESpecialite              gender group                 \n",
       "Arts            Fine Arts & Architecture F      asian       0.000094  \n",
       "                                                black       0.000080  \n",
       "                                                hispanic    0.000047  \n",
       "                                                white       0.000608  \n",
       "                                         M      asian       0.000087  \n",
       "...                                                              ...  \n",
       "Social Sciences Sociology                F      white       0.001816  \n",
       "                                         M      asian       0.000321  \n",
       "                                                black       0.000302  \n",
       "                                                hispanic    0.000202  \n",
       "                                                white       0.002286  \n",
       "\n",
       "[1136 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discipline_gender_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average citations by group & specialite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = us_papers_race[['EDiscipline','ESpecialite','gender','cit_rel_all_IAC','cit_all_IAC','white', 'hispanic', 'black', 'asian']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset = [\"cit_all_IAC\"], inplace=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.melt(df, id_vars=['EDiscipline','ESpecialite','gender','cit_all_IAC','cit_rel_all_IAC'], value_vars=['white', 'hispanic', 'black', 'asian'],\n",
    "       var_name='race', value_name='weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_median(data, weights, quantile=.5):\n",
    "     # Check the data\n",
    "    if not isinstance(data, np.matrix):\n",
    "        data = np.asarray(data)\n",
    "    if not isinstance(weights, np.matrix):\n",
    "        weights = np.asarray(weights)\n",
    "    nd = data.ndim\n",
    "    if nd != 1:\n",
    "        raise TypeError(\"data must be a one dimensional array\")\n",
    "    ndw = weights.ndim\n",
    "    if ndw != 1:\n",
    "        raise TypeError(\"weights must be a one dimensional array\")\n",
    "    if data.shape != weights.shape:\n",
    "        raise TypeError(\"the length of data and weights must be the same\")\n",
    "    if ((quantile > 1.) or (quantile < 0.)):\n",
    "        raise ValueError(\"quantile must have a value between 0. and 1.\")\n",
    "    # Sort the data\n",
    "    ind_sorted = np.argsort(data)\n",
    "    sorted_data = data[ind_sorted]\n",
    "    sorted_weights = weights[ind_sorted]\n",
    "    # Compute the auxiliary arrays\n",
    "    Sn = np.cumsum(sorted_weights)\n",
    "    # TODO: Check that the weights do not sum zero\n",
    "    #assert Sn != 0, \"The sum of the weights must not be zero\"\n",
    "    Pn = (Sn-0.5*sorted_weights)/Sn[-1]\n",
    "    # Get the value of the weighted median\n",
    "    return np.interp(quantile, Pn, sorted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_wmedian(group):\n",
    "    data = group['cit_all_IAC']\n",
    "    weights = group['weights']\n",
    "    median = weighted_median(data, weights, quantile=.5)\n",
    "    return median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations_agg = df.groupby(['EDiscipline','ESpecialite','gender', 'race']).apply(grouped_wmedian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "discipline_gender_agg['median_citations'] = citations_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(discipline_gender_agg.median_citations.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_wmean(group, citation_type='cit_all_IAC'):\n",
    "    values = group[citation_type]\n",
    "    weights = group['weights']\n",
    "    mean = np.average(values, weights=weights)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_citations  = df[df.weights!=0].groupby(['EDiscipline','ESpecialite','gender', 'race']).apply(grouped_wmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EDiscipline      ESpecialite               gender  race    \n",
       "Arts             Fine Arts & Architecture  F       asian       0.882907\n",
       "                                                   black       0.527940\n",
       "                                                   hispanic    0.575744\n",
       "                                                   white       0.505146\n",
       "                                           M       asian       1.030127\n",
       "                                                                 ...   \n",
       "Social Sciences  Sociology                 F       white       9.026525\n",
       "                                           M       asian       8.603166\n",
       "                                                   black       8.677733\n",
       "                                                   hispanic    8.339761\n",
       "                                                   white       9.745542\n",
       "Length: 1136, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rel_citations  = df[df.weights!=0].groupby(['EDiscipline','ESpecialite','gender', 'race']).apply(lambda x: grouped_wmean(x,'cit_rel_all_IAC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EDiscipline      ESpecialite               gender  race    \n",
       "Arts             Fine Arts & Architecture  F       asian       1.399342\n",
       "                                                   black       0.932253\n",
       "                                                   hispanic    0.899143\n",
       "                                                   white       0.917584\n",
       "                                           M       asian       1.664927\n",
       "                                                                 ...   \n",
       "Social Sciences  Sociology                 F       white       1.444623\n",
       "                                           M       asian       1.287778\n",
       "                                                   black       1.363124\n",
       "                                                   hispanic    1.342150\n",
       "                                                   white       1.485567\n",
       "Length: 1136, dtype: float64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_rel_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "discipline_gender_agg['mean_citations'] = mean_citations\n",
    "discipline_gender_agg['mean_rel_citations'] = mean_rel_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "discipline_gender_agg.to_csv('../results/discipline_gender_agg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leiden Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "subfields = pd.read_csv('/data/WOS/US/subfields.txt', delimiter='\\t')\n",
    "titles = pd.read_csv('/data/WOS/US/titles.txt', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_papers_race2 = us_papers_race.merge(titles, how='inner', left_on='id_art', right_on='id_Art')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_papers_race2 = us_papers_race2.merge(subfields, how='inner', left_on='ITEMID', right_on='ut')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-60-6251f965e6be>:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  subfields_race = us_papers_race2.groupby('cluster_id1')['white','hispanic','black','asian'].mean().reset_index()\n"
     ]
    }
   ],
   "source": [
    "subfields_race = us_papers_race2.groupby('cluster_id1')['white','hispanic','black','asian'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "subfields_race.to_csv('../results/leiden_clusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
